<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.60.1" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Information theory and coding  &middot; Labyrinth</title>
  <meta name="description" content="" />

  
  <link type="text/css" rel="stylesheet" href="https://evelyn92.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://evelyn92.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://evelyn92.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://evelyn92.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css"/>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js"></script>
  
  
</head>

  <body class=" ">
  <aside class="sidebar ">
  <div class="container sidebar-sticky ">
    <div class="sidebar-about">
      <a href="https://evelyn92.github.io/"><h1>Labyrinth</h1></a>
      <p class="lead">
      An elegant open blog for <a href="http://hugo.spf13.com">Evelyn Jia</a> made by <a href="http://twitter.com/mdo">@Evelyn92</a>. 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://evelyn92.github.io/">Home</a> </li>
        <li><a href="https://evelyn92.github.io/">Aps </a> </li>
        <li><a href="https://evelyn92.github.io/">Front-end</a> </li>
        
      </ul>
    </nav>

    <p>&copy; 2019. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
  <h1>Information theory and coding </h1>
  <time datetime=2019-12-05T11:02:11&#43;0800 class="post-date">Thu, Dec 5, 2019</time>
  <h1 id="1-introduction">1. Introduction</h1>
<p>It is a discipline that applies the methods of ==probability theory==, ==stochastic processes== , ==mathematical statistics== , and ==modern algebra==  to study general laws in information transmission, extraction, and processing.</p>
<h2 id="11-it-researches-the-basic-problem-of">1.1 It researches the basic problem of</h2>
<ul>
<li>Information theory—Measurement of information</li>
<li>Distortion-Free Source Coding Theorem-Shannon's First Theorem</li>
<li>Channel coding theorem-Shannon's second theorem</li>
<li>Source coding, channel coding</li>
</ul>
<h2 id="12-some-important-concepts">1.2 Some important concepts</h2>
<ul>
<li>
<p>Sample space: The various states in which something can occur</p>
</li>
<li>
<p>Probability space
$$
\left[
\begin{matrix}X\<br>
P(x)
\end{matrix}
\right]=
\left[
\begin{matrix}
a_1 &amp; a_2 &amp; &hellip;&amp;a_3 \</p>
<pre><code>    p(a_1) &amp; p(a_2)&amp;... &amp; p(a_3)
\end{matrix} 
\right]
</code></pre>
<p>$$</p>
</li>
<li>
<p>Probability measure: Specify a probability for each possible choice of message</p>
</li>
<li>
<p>Prior Probability p (xi): the probability of choosing the symbol xi .</p>
</li>
</ul>
<h2 id="13-purpose-of-information-theory">1.3 Purpose of Information Theory</h2>
<ul>
<li>Find common laws in the process of information transmission</li>
<li>Improve the ==reliability== , ==effectiveness== , ==confidentiality==  and ==authentication==  of information transmission to optimize the information transmission system.</li>
</ul>
<h1 id="2-measure-of-discrete-information">2. Measure of discrete information</h1>
<h2 id="21-selfinformation-and-mutual-information">2.1 Self-information and mutual information</h2>
<h3 id="211-self-information">2.1.1 Self information</h3>
<ul>
<li>
<p>Self information: Self-information of event $x = a_i$  in event set X</p>
<p>$$I_x(a_i)=-logP_x(a_i)$$</p>
<p>The concept includes two meanings:</p>
<ul>
<li>before the event happens-&gt;the uncertainty of the event</li>
<li>after the event happens-&gt;Amount of information contained in the event</li>
</ul>
</li>
<li>
<p>Joint self-information: Self-information of event $x = a_i, y = b_j$   in event set X Y</p>
<p>$$I_{XY}(a_i,b_j)=-logP_{XY}(a_i,b_j)$$</p>
</li>
<li>
<p>Conditional self-information: Given $y=b_j$ , the self-information of event $x=a_i$</p>
<p>$$I_{X|Y}(a_i|b_j)=-logP_{x|y}(a_i|b_j)$$</p>
<p>The concept includes two meanings:</p>
<ul>
<li>before the event happens-&gt;the uncertainty of the event</li>
<li>after the event happens-&gt;Amount of information contained in the event</li>
</ul>
</li>
</ul>
<h3 id="212-mutual-information">2.1.2 Mutual information</h3>
<ul>
<li>
<p>Mutual information: Mutual information between discrete random events</p>
<p>$$I(x;y)=log\frac{p(x/y)}{p(x)}$$</p>
<p>Calculated:</p>
<p>$$I(x;y)=I(x)-I(x|y)$$                 <img src="first.assets/2-1-2.png" alt="avatar"></p>
</li>
<li>
<p>The properties of mutual information</p>
<ul>
<li>Reciprocity: $$I(x;y)=I(y;x)$$</li>
<li>The mutual information between any two events cannot be greater than the self-information of any one event</li>
</ul>
</li>
<li>
<p>Conditional mutual information</p>
<p>​	skip</p>
</li>
</ul>
<h2 id="22-information-entropy">2.2 Information entropy</h2>
<h3 id="221-the-definition-and-calculation-of-information-entropy">2.2.1 The definition and calculation of information entropy</h3>
<p>$$H(X)=E_{p(x)}[I(x)]=-\sum_{x}{p(x)logp(x)}$$</p>
<ul>
<li>
<p>meanings:</p>
<ul>
<li>
<p>Before the source output-&gt; the average uncertainty of source</p>
</li>
<li>
<p>After the source output-&gt;Average amount of information provided by a source symbol</p>
</li>
<li>
<p>Source randomness: the larger the H(x), the larger the randomness</p>
</li>
</ul>
</li>
</ul>
<h3 id="222-the-conditional-entropy">2.2.2 The conditional entropy</h3>
<p>The average of conditional self information in the joint set X Y</p>
<p>$$H(Y/X)=E_{p(xy)}[I(y/x)]\=\sum_{x}{p(x)[-\sum_{y}{p(y/x)logp(y/x)}]}\=-\sum_{x}{\sum_{y}{p(y/x)logp(y/x)}}$$</p>
<h3 id="223-the-fundamental-property-of-entropy">2.2.3 The fundamental property of entropy</h3>
<ul>
<li>
<p>Concave function&ndash;cap:  $E[f(x)]\leq{f[E(x)]}$</p>
</li>
<li>
<p>Information divergence: $D(P//Q)=\sum_{x}{P(x)log\frac{P(x)}{Q(x)}}$  Average number of extra bits required to ==encode source X subject to P==  using ==distribution Q-based coding==</p>
</li>
<li>
<p>The fundamental properties</p>
<ul>
<li>
<p>Non-negative</p>
</li>
<li>
<p>Additivity: $$H(XY)=H(X)+H(Y|X) \H(X_1X_2&hellip;X_N)=H(X_1)+H(X_2|X_1)+&hellip;+H(X_N|X_1&hellip;X_{N-1)}$$</p>
</li>
<li>
<p>Discrete maximum entropy theorem:</p>
<p>For discrete random sources, the entropy reaches its maximum when the events occur with equal probability</p>
<p>$H(X)\leq{log n}$</p>
</li>
</ul>
</li>
<li>
<p>The relationship between various types of entropy</p>
<p>The joint entropy not larger than the sum of the information entropy</p>
<p>$$H(X_1X_2&hellip;X_N)\leq{\sum_{i=1}^{N}H(X_i)}$$</p>
</li>
</ul>
<h2 id="23-average-mutual-information">2.3 Average mutual information</h2>
<h3 id="231-definition">2.3.1 Definition:</h3>
<ul>
<li>
<p>The average mutual information between set X Y:</p>
<p>$I(X;Y)=\sum_x{p(x)I(Y;x)}$</p>
</li>
<li>
<p>The relationship between average mutual information and entropy:</p>
<p><img src="first.assets/2-3-1.png" alt="avatar"></p>
</li>
</ul>
<h3 id="232-properties">2.3.2 Properties</h3>
<ul>
<li>non-negative</li>
<li>Reciprocity:$ I(X;Y)=I(Y;X)$</li>
</ul>
<h3 id="233-average-conditional-mutual-information">2.3.3 Average conditional mutual information</h3>
<p>skip</p>
<h1 id="3-discrete-source">3. Discrete source</h1>
<h2 id="31-classification-of-discrete-sources-and-mathematical-model">3.1 Classification of discrete sources and mathematical model</h2>
<h3 id="311-classification">3.1.1 Classification</h3>
<ul>
<li>Memory /memoryless source: based on dependencies between input symbols</li>
<li>Stationary source/Non-stationary source</li>
</ul>
<h3 id="312-mathematical-model">3.1.2 Mathematical model</h3>
<ul>
<li>
<p>Single symbol discrete memoryless source</p>
<p>$$\left[\begin{matrix}X\P\end{matrix}\right]=\left[\begin{matrix}a_1&amp;&hellip;&amp;a_n\p(a_1)&amp;&hellip;&amp;p(a_n)\end{matrix}\right]$$</p>
<p>$p(a_i)\geq{0}$      	$\sum^n_{i=1}{p(a_i)}=1$</p>
</li>
<li>
<p>Multidimensional discrete memoryless source mathematical model</p>
<p>$$\left[\begin{matrix}X^N\P\end{matrix}\right]=\left[\begin{matrix}a_1&amp;&hellip;&amp;a_M\p(a_1)&amp;&hellip;&amp;p(a_M)\end{matrix}\right]$$</p>
<p>The N-dimensional extended source of Source X: $X^N=X_1X_2&hellip;X_N$</p>
<p>Example:</p>
<p><!-- raw HTML omitted --></p>
</li>
</ul>
<h3 id="313-the-mathematic-model-of-discrete-memory-source">3.1.3 The mathematic model of discrete memory source</h3>
<ul>
<li>Discrete Markov source
<ul>
<li>Memory source</li>
<li>Markov sources can be defined from the concept of finite state machines.</li>
<li>The finite state machine contains both the transition relationship between ==states and the relationship== between ==outputs and states==.</li>
</ul>
</li>
</ul>
<h2 id="32-the-entropy-of-discrete-memoryless-source">3.2 The entropy of discrete memoryless source</h2>
<h3 id="321-single-symbol">3.2.1 Single symbol</h3>
<p>$$H(X)=-plogp-(1-p)log(1-p)=H(p)$$</p>
<p>​                          <img src="first.assets/image-20191205105353706.png" alt="image-20191205105353706"></p>
<h3 id="322-the-entropy-of-discrete-memoryless-ndimension-extended-source">3.2.2 The entropy of discrete memoryless N-dimension extended source</h3>
<ul>
<li>
<p>The theorem</p>
<p>$H(X^N)=NH(X)$</p>
</li>
</ul>
<h2 id="33-the-entropy-of-discrete-stationary-source">3.3 The entropy of discrete stationary source</h2>
<h3 id="331-discrete-stationary-source">3.3.1 Discrete stationary source</h3>
<ul>
<li>
<p>Definition</p>
<ul>
<li>Source X has finite symbol set $A={a_1,a_2,&hellip;,a_n}$</li>
<li>Source generate random sequence${x_i}$  $i=&hellip;,1,2,&hellip;$</li>
<li><img src="first.assets/image-20191205110918760.png" alt="image-20191205110918760"></li>
</ul>
</li>
<li>
<p>Feature</p>
<ul>
<li>
<p>Statistical characteristics are not related to the passage of time(doesn't change)</p>
<p>$$p(x_i,x_{i+1},&hellip;,x_{i+N})=p(x_j,x_{j+1},&hellip;,x_{j+N})$$</p>
</li>
<li>
<p>For stationary source, conditional probability is also stationary</p>
<p>$$P(x_{i+N}/x_i,x_{i+1},&hellip;,x_{i+N-1})=P(x_{j+N}/x_j,x_{j+1},&hellip;x_{j+N-1})$$</p>
</li>
<li>
<p>The entropy of stationary source is not related to the start of the time</p>
<p>$H(X_iX_{i+1}&hellip;X_{i+N})=H(X_jX_{j+1}&hellip;X_{j+N})$</p>
<p>$H(X_{i+N}/X_i,X_{i+1},&hellip;,X_{i+N-1})=H(X_{j+N}/X_j,X_{j+1},&hellip;,X_{j+N-1})$</p>
</li>
</ul>
</li>
</ul>
<h3 id="332-the-entropy-of-discrete-stationary-memory-source">3.3.2 The entropy of discrete stationary memory source</h3>
<ul>
<li>
<p>Non-increasing principle: $H(X^N)\leq{NH(X_1)}$</p>
<p>The equation holds only when it is memoryless source.</p>
</li>
</ul>
<h2 id="34-finite-state-markov-chain">3.4 Finite state Markov chain</h2>
<h3 id="341-the-basic-concept-of-markov-chain">3.4.1 The basic concept of Markov chain</h3>
<ul>
<li>
<p>Definition</p>
<p>In a random sequence: ${X_n,n\geq{0}}$  , every random variable $x_n(n\geq{1})$ depends only on $x_{n-1}$</p>
</li>
<li>
<p>Some concepts</p>
<ul>
<li>
<p>The current state of the first-order Markov chain is only related to the previous state.</p>
</li>
<li>
<p>Similarly, .. n-order&hellip;.previous n states.</p>
</li>
<li>
<p>State transition probability:</p>
<p>For discrete moments$m$ $n$ , the corresponding transition probability:</p>
<p>$p(x_n=j/x_m=j)=p_{ij}(m,n)$</p>
<p>Represents the probability of transitioning from the i state at time m to the j state at time n</p>
</li>
</ul>
</li>
<li>
<p>Homogeneous Markov chains</p>
<ul>
<li>
<p>Example: the differences between the homogeneous one and before</p>
<p>$p_{ij}(m,n)$ may be change over the different start of time m. But for the homogeneous one, it  can be constant.</p>
</li>
<li>
<p>The transition probability is not related to  the starting time, and it is stable. $p_{ij}(m,m+1)=p(x_{m+1}=j/x_m=i)=p_{ij}$</p>
</li>
<li>
<p>Transition probability matrix</p>
<p>$$[P]=[P_{ij}]=\left[\begin{matrix}p_{11}&amp;p_{12}&amp;&hellip;&amp;p_{1J}\p_{21}&amp;p_{22}&amp;&hellip;&amp;p_{2J}\p_{J1}&amp;p_{J2}&amp;&hellip;&amp;p_{JJ}\end{matrix}\right]$$</p>
</li>
<li></li>
<li>
<p>State transition diagram</p>
<p><img src="first.assets/image-20191205161417098.png" alt="image-20191205161417098"></p>
</li>
</ul>
</li>
</ul>
<h2 id="35-markov-sources">3.5 Markov Sources</h2>
<h3 id="351-the-fundamental-concepts-of-markov-sources">3.5.1 The fundamental concepts of Markov sources</h3>
<ul>
<li>Definition: The probability of the output symbol at current moment depends only on the current source state.</li>
<li>Current source state depends only on the previous source state and previous output symbol at the previous moment.</li>
</ul>
<h3 id="352-the-model">3.5.2 The model</h3>
<p>skip</p>
<h3 id="353-the-calculation-of-symbol-entropy">3.5.3 The calculation of symbol entropy</h3>
<p>skip</p>
<h2 id="36-source-correlation-and-source-redundancy">3.6 Source correlation and Source redundancy</h2>
<h3 id="361-correlation">3.6.1 Correlation</h3>
<ul>
<li>Definition: Degree of dependency between source symbols</li>
<li>The ==average self-information==  provided by the source symbol decreases as the ==length of the dependency relationship==  between symbols increases.</li>
</ul>
<h3 id="362-redundancy">3.6.2 Redundancy</h3>
<ul>
<li>
<p>Source efficiency:</p>
<p>$$</p>
</li>
<li>
<p>Source surplus:</p>
</li>
</ul>
<h3 id="heading"></h3>

</div>


    </main>

    
  </body>
</html>
