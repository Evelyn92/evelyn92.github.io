<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.60.1" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title> &middot; Labyrinth</title>
  <meta name="description" content="" />

  
  <link type="text/css" rel="stylesheet" href="https://evelyn92.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://evelyn92.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://evelyn92.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://evelyn92.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css"/>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js"></script>
  
  
</head>

  <body class=" ">
  <aside class="sidebar ">
  <div class="container sidebar-sticky ">
    <div class="sidebar-about">
      <a href="https://evelyn92.github.io/"><h1>Labyrinth</h1></a>
      <p class="lead">
      An elegant open blog for <a href="http://hugo.spf13.com">Evelyn Jia</a> made by <a href="http://twitter.com/mdo">@Evelyn92</a>. 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://evelyn92.github.io/">Home</a> </li>
        <li><a href="https://evelyn92.github.io/">Aps </a> </li>
        <li><a href="https://evelyn92.github.io/">Front-end</a> </li>
        
      </ul>
    </nav>

    <p>&copy; 2019. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
  <h1></h1>
  <time datetime=0001-01-01T00:00:00Z class="post-date">Mon, Jan 1, 0001</time>
  <h1 id="information-theory-and-coding">Information theory and coding</h1>
<hr>
<h1 id="introduction">Introduction</h1>
<p>It is a discipline that applies the methods of ==probability theory==, ==stochastic processes== , ==mathematical statistics== , and ==modern algebra==  to study general laws in information transmission, extraction, and processing.</p>
<h2 id="it-researches-the-basic-problem-of">It researches the basic problem of</h2>
<ul>
<li>Information theory—Measurement of information</li>
<li>Distortion-Free Source Coding Theorem-Shannon's First Theorem</li>
<li>Channel coding theorem-Shannon's second theorem</li>
<li>Source coding, channel coding</li>
</ul>
<h2 id="some-important-concepts">Some important concepts</h2>
<ul>
<li>
<p>Sample space: The various states in which something can occur</p>
</li>
<li>
<p>Probability space
$$
\left[	\begin{matrix}X\	P(x)	\end{matrix}     \right]=\left[    \begin{matrix}        a_1 &amp; a_2 &amp; &hellip;&amp;a_3 \              p(a_1) &amp; p(a_2)&amp;&hellip; &amp; p(a_3)    \end{matrix}     \right]
$$</p>
</li>
<li>
<p>Probability measure: Specify a probability for each possible choice of message</p>
</li>
<li>
<p>Prior Probability p (xi): the probability of choosing the symbol xi .</p>
</li>
</ul>
<h2 id="purpose-of-information-theory">Purpose of Information Theory</h2>
<ul>
<li>Find common laws in the process of information transmission</li>
<li>Improve the ==reliability== , ==effectiveness== , ==confidentiality==  and ==authentication==  of information transmission to optimize the information transmission system.</li>
</ul>
<h1 id="measure-of-discrete-information">Measure of discrete information</h1>
<h2 id="selfinformation-and-mutual-information">Self-information and mutual information</h2>
<h3 id="self-information">Self information</h3>
<ul>
<li>
<p>Self information: Self-information of event $x = a_i$  in event set X</p>
<p>$$I_x(a_i)=-logP_x(a_i)$$</p>
<p>The concept includes two meanings:</p>
<ul>
<li>before the event happens-&gt;the uncertainty of the event</li>
<li>after the event happens-&gt;Amount of information contained in the event</li>
</ul>
</li>
<li>
<p>Joint self-information: Self-information of event $x = a_i, y = b_j$   in event set X Y</p>
<p>$$I_{XY}(a_i,b_j)=-logP_{XY}(a_i,b_j)$$</p>
</li>
<li>
<p>Conditional self-information: Given $y=b_j$ , the self-information of event $x=a_i$</p>
<p>$$I_{X|Y}(a_i|b_j)=-logP_{x|y}(a_i|b_j)$$</p>
<p>The concept includes two meanings:</p>
<ul>
<li>before the event happens-&gt;the uncertainty of the event</li>
<li>after the event happens-&gt;Amount of information contained in the event</li>
</ul>
</li>
</ul>
<h3 id="mutual-information">Mutual information</h3>
<ul>
<li>
<p>Mutual information: Mutual information between discrete random events</p>
<p>$$I(x;y)=log\frac{p(x/y)}{p(x)}$$</p>
<p>Calculated:</p>
<p>$$I(x;y)=I(x)-I(x|y)$$                   <img src="C:%5CUsers%5Cevyji%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191204204825312.png" alt="image-20191204204825312"></p>
</li>
<li>
<p>The properties of mutual information</p>
<ul>
<li>Reciprocity: $$I(x;y)=I(y;x)$$</li>
<li>The mutual information between any two events cannot be greater than the self-information of any one event</li>
</ul>
</li>
<li>
<p>Conditional mutual information</p>
<p>​	skip</p>
</li>
</ul>
<h2 id="information-entropy">Information entropy</h2>
<h3 id="the-definition-and-calculation-of-information-entropy">The definition and calculation of information entropy</h3>
<p>$$H(X)=E_{p(x)}[I(x)]=-\sum_{x}{p(x)logp(x)}$$</p>
<ul>
<li>
<p>meanings:</p>
<ul>
<li>
<p>Before the source output-&gt; the average uncertainty of source</p>
</li>
<li>
<p>After the source output-&gt;Average amount of information provided by a source symbol</p>
</li>
<li>
<p>Source randomness: the larger the H(x), the larger the randomness</p>
</li>
</ul>
</li>
</ul>
<h3 id="the-conditional-entropy">The conditional entropy</h3>
<p>The average of conditional self information in the joint set X Y</p>
<p>$$H(Y/X)=E_{p(xy)}[I(y/x)]\=\sum_{x}{p(x)[-\sum_{y}{p(y/x)logp(y/x)}]}\=-\sum_{x}{\sum_{y}{p(y/x)logp(y/x)}}$$</p>
<h3 id="the-fundamental-property-of-entropy">The fundamental property of entropy</h3>
<ul>
<li>
<p>Concave function&ndash;cap:  $E[f(x)]\leq{f[E(x)]}$</p>
</li>
<li>
<p>Information divergence: $D(P//Q)=\sum_{x}{P(x)log\frac{P(x)}{Q(x)}}$  Average number of extra bits required to ==encode source X subject to P==  using ==distribution Q-based coding==</p>
</li>
<li>
<p>The fundamental properties</p>
<ul>
<li>
<p>Non-negative</p>
</li>
<li>
<p>Additivity: $$H(XY)=H(X)+H(Y|X) \H(X_1X_2&hellip;X_N)=H(X_1)+H(X_2|X_1)+&hellip;+H(X_N|X_1&hellip;X_{N-1)}$$</p>
</li>
<li>
<p>Discrete maximum entropy theorem:</p>
<p>For discrete random sources, the entropy reaches its maximum when the events occur with equal probability</p>
<p>$H(X)\leq{log n}$</p>
</li>
</ul>
</li>
<li>
<p>The relationship between various types of entropy</p>
<p>The joint entropy not larger than the sum of the information entropy</p>
<p>$$H(X_1X_2&hellip;X_N)\leq{\sum_{i=1}^{N}H(X_i)}$$</p>
</li>
</ul>
<h2 id="average-mutual-information">Average mutual information</h2>
<h3 id="definition">Definition:</h3>
<ul>
<li>
<p>The average mutual information between set X Y:</p>
<p>$I(X;Y)=\sum_x{p(x)I(Y;x)}$</p>
</li>
<li>
<p>The relationship between average mutual information and entropy:</p>
<p><img src="C:%5CUsers%5Cevyji%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20191204220107138.png" alt="image-20191204220107138"></p>
</li>
</ul>
<h3 id="properties">Properties</h3>
<ul>
<li>non-negative</li>
<li>Reciprocity:$ I(X;Y)=I(Y;X)$</li>
</ul>
<h3 id="average-conditional-mutual-information">Average conditional mutual information</h3>

</div>


    </main>

    
  </body>
</html>
